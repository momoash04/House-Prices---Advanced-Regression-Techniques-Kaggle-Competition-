{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Load data\n\ntrain_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest_df  = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\ntest_ids = test_df[\"Id\"].copy()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  feature engineering\n\nfor df in [train_df, test_df]:\n    df[\"TotalSF\"] = df[\"TotalBsmtSF\"].fillna(0) + df[\"1stFlrSF\"].fillna(0) + df[\"2ndFlrSF\"].fillna(0)\n    df[\"TotalBath\"] = (\n        df[\"FullBath\"].fillna(0) + 0.5*df[\"HalfBath\"].fillna(0) +\n        df[\"BsmtFullBath\"].fillna(0) + 0.5*df[\"BsmtHalfBath\"].fillna(0)\n    )\n    df[\"HouseAge\"] = df[\"YrSold\"] - df[\"YearBuilt\"]\n    df[\"RemodAge\"] = df[\"YrSold\"] - df[\"YearRemodAdd\"]\n\nfor df in [train_df, test_df]:\n    df[\"GrLivArea_Qual\"] = df[\"GrLivArea\"] * df[\"OverallQual\"]\n    df[\"TotalSF_Qual\"]   = df[\"TotalSF\"]   * df[\"OverallQual\"]\n    df[\"LotArea_Qual\"]   = df[\"LotArea\"]   * df[\"OverallQual\"]\n    df[\"GarageCars_SF\"]  = df[\"GarageCars\"] * df[\"TotalSF\"]\n\n# Drop Id\ntrain_df = train_df.drop(columns=[\"Id\"])\ntest_df  = test_df.drop(columns=[\"Id\"])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Missing values\n\nfor col in train_df.select_dtypes(include=[\"float64\", \"int64\"]).columns:\n    train_df[col] = train_df[col].fillna(train_df[col].median())\nfor col in train_df.select_dtypes(include=[\"object\"]).columns:\n    train_df[col] = train_df[col].fillna(\"Missing\")\n\nfor col in test_df.select_dtypes(include=[\"float64\", \"int64\"]).columns:\n    test_df[col] = test_df[col].fillna(train_df[col].median())\nfor col in test_df.select_dtypes(include=[\"object\"]).columns:\n    test_df[col] = test_df[col].fillna(\"Missing\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Features and target\n\nX_df = train_df.drop(columns=[\"SalePrice\"])\ny    = np.log1p(train_df[\"SalePrice\"])\n\nnum_cols = X_df.select_dtypes(include=[\"int64\", \"float64\"]).columns\ncat_cols = X_df.select_dtypes(include=[\"object\"]).columns\n\nnum_pipe = Pipeline(steps=[('scaler', StandardScaler())])\ncat_pipe = Pipeline(steps=[('encoder', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_pipe, num_cols),\n        ('cat', cat_pipe, cat_cols)\n    ]\n)\n\n# Fit on train \nX_all   = preprocessor.fit_transform(X_df)\nX_test  = preprocessor.transform(test_df)\n\n# Get feature names for pruning\nfeat_names = preprocessor.get_feature_names_out()\n\n# Convert to arrays \nX_all  = X_all.toarray()\nX_test = X_test.toarray()\n\n\n# Feature importance pruning (LightGBM on full train)\n\nlgb_probe = lgb.LGBMRegressor(\n    n_estimators=2000, learning_rate=0.05,\n    num_leaves=31, subsample=0.7, colsample_bytree=0.7,\n    random_state=42\n)\nlgb_probe.fit(X_all, y)\nimportances = lgb_probe.feature_importances_.astype(float)\nimportances = importances / (importances.sum() + 1e-12)\n\n# Rank features\nidx_sorted = np.argsort(importances)[::-1]\ncum_imp = np.cumsum(importances[idx_sorted])\n\n# Choose K \nK_floor = min(300, len(idx_sorted))\nK_cum = int(np.searchsorted(cum_imp, 0.95) + 1)\nK = max(K_floor, K_cum)\nK = min(K, len(idx_sorted))  \n\nkeep_idx = idx_sorted[:K]\nkeep_idx_sorted = np.sort(keep_idx)  \n\nX_all_pruned  = X_all[:, keep_idx_sorted]\nX_test_pruned = X_test[:, keep_idx_sorted]\nfeat_names_pruned = feat_names[keep_idx_sorted]\n\nprint(f\"Kept top {K} features out of {len(feat_names)} by LightGBM importance (~95% cumulative gain).\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model\n\ndef build_nn(input_dim, seed=None):\n    if seed is not None:\n        tf.keras.utils.set_random_seed(seed)\n    l2 = tf.keras.regularizers.l2(5e-4)\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=l2, input_shape=(input_dim,)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n\n        tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=l2),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.25),\n\n        tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=l2),\n        tf.keras.layers.Dropout(0.2),\n\n        tf.keras.layers.Dense(1)  # regression (log-price)\n    ])\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n        loss=tf.keras.losses.Huber(delta=1.0)\n    )\n    return model\n\nxgb_params = dict(\n    n_estimators=5000,       \n    learning_rate=0.03,\n    max_depth=4,\n    subsample=0.7,\n    colsample_bytree=0.7,\n    reg_alpha=0.1,\n    reg_lambda=1.0,\n    random_state=42,\n    early_stopping_rounds=200\n)\n\nlgb_params = dict(\n    n_estimators=5000,\n    learning_rate=0.03,\n    num_leaves=32,\n    subsample=0.7,\n    colsample_bytree=0.7,\n    reg_alpha=0.1,\n    reg_lambda=1.0,\n    random_state=42\n)\n\ncat_params = dict(\n    iterations=5000,\n    learning_rate=0.03,\n    depth=6,\n    loss_function='RMSE',\n    random_seed=42,\n    verbose=False,\n    od_type='Iter',\n    od_wait=200\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 10-Fold \n\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nn = X_all_pruned.shape[0]\nm = X_test_pruned.shape[0]\n\noof_nn  = np.zeros(n)\noof_xgb = np.zeros(n)\noof_lgb = np.zeros(n)\noof_cat = np.zeros(n)\n\ntest_pred_nn  = np.zeros((m, 10))\ntest_pred_xgb = np.zeros((m, 10))\ntest_pred_lgb = np.zeros((m, 10))\ntest_pred_cat = np.zeros((m, 10))\n\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_all_pruned), start=1):\n    print(f\"\\n===== FOLD {fold}/10 =====\")\n    X_tr, X_va = X_all_pruned[tr_idx], X_all_pruned[va_idx]\n    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n    \n    preds_va_bag = []\n    preds_te_bag = []\n    for seed in [fold*13, fold*29]:  \n        nn = build_nn(X_tr.shape[1], seed=seed)\n        cb_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n        cb_rlr   = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-6)\n        nn.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n               epochs=500, batch_size=32, verbose=0, callbacks=[cb_early, cb_rlr])\n        preds_va_bag.append(nn.predict(X_va, verbose=0).flatten())\n        preds_te_bag.append(nn.predict(X_test_pruned, verbose=0).flatten())\n\n    va_nn  = np.mean(preds_va_bag, axis=0)\n    te_nn  = np.mean(preds_te_bag, axis=0)\n    oof_nn[va_idx] = va_nn\n    test_pred_nn[:, fold-1] = te_nn\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # --- XGBoost ---\n    xgb_model = xgb.XGBRegressor(**xgb_params)\n    xgb_model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n    oof_xgb[va_idx] = xgb_model.predict(X_va)\n    test_pred_xgb[:, fold-1] = xgb_model.predict(X_test_pruned)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # --- LightGBM ---\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n    lgb_model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n                  callbacks=[lgb.early_stopping(200), lgb.log_evaluation(0)])\n    oof_lgb[va_idx] = lgb_model.predict(X_va)\n    test_pred_lgb[:, fold-1] = lgb_model.predict(X_test_pruned)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # --- CatBoost ---\n    cat = CatBoostRegressor(**cat_params)\n    cat.fit(X_tr, y_tr, eval_set=(X_va, y_va), verbose=False)\n    oof_cat[va_idx] = cat.predict(X_va)\n    test_pred_cat[:, fold-1] = cat.predict(X_test_pruned)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# OOF RMSE diagnostics\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred, squared=False)\n\nrmse_nn  = rmse(y, oof_nn)\nrmse_xgb = rmse(y, oof_xgb)\nrmse_lgb = rmse(y, oof_lgb)\nrmse_cat = rmse(y, oof_cat)\nprint(\"\\nOOF RMSE (log-space):\")\nprint(f\"NN:   {rmse_nn:.6f}\")\nprint(f\"XGB:  {rmse_xgb:.6f}\")\nprint(f\"LGBM: {rmse_lgb:.6f}\")\nprint(f\"CAT:  {rmse_cat:.6f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Meta-model stacking (Ridge)\n\nstack_X = np.vstack([oof_nn, oof_xgb, oof_lgb, oof_cat]).T\nstack_T = np.vstack([\n    test_pred_nn.mean(axis=1),\n    test_pred_xgb.mean(axis=1),\n    test_pred_lgb.mean(axis=1),\n    test_pred_cat.mean(axis=1)\n]).T\n\nmeta = Ridge(alpha=1.0, random_state=42)\nmeta.fit(stack_X, y)\noof_stack = meta.predict(stack_X)\nstack_rmse = rmse(y, oof_stack)\ntest_stack_log = meta.predict(stack_T)\n\nprint(f\"\\nOOF RMSE (Stacked Ridge): {stack_rmse:.6f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimized convex blending (non-negative weights sum to 1)\n\nmodels_oof = np.vstack([oof_nn, oof_xgb, oof_lgb, oof_cat])  # shape (4, n)\nmodels_test = np.vstack([\n    test_pred_nn.mean(axis=1),\n    test_pred_xgb.mean(axis=1),\n    test_pred_lgb.mean(axis=1),\n    test_pred_cat.mean(axis=1)\n])  \n\nbest_rmse = 1e9\nbest_w = None\n\nstep = 0.05\nweights = np.arange(0.0, 1.0 + 1e-9, step)\n\nfor w1 in weights:\n    for w2 in weights:\n        for w3 in weights:\n            w4 = 1.0 - (w1 + w2 + w3)\n            if w4 < -1e-9 or w4 > 1.0:  \n                continue\n            if w4 < 0:  \n                w4 = 0.0\n            # normalize to sum 1\n            s = w1 + w2 + w3 + w4\n            if s == 0:\n                continue\n            w = np.array([w1, w2, w3, w4]) / s\n            blend_oof = (w[:, None] * models_oof).sum(axis=0)\n            score = rmse(y, blend_oof)\n            if score < best_rmse:\n                best_rmse = score\n                best_w = w\n\nprint(f\"\\nBest OOF RMSE (Optimized Blend): {best_rmse:.6f} with weights [NN, XGB, LGB, CAT] = {best_w}\")\n\nblend_test_log = (best_w[:, None] * models_test).sum(axis=0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Choose between Stacking vs Optimized Blend (lower OOF RMSE wins)\n\nuse_stack = stack_rmse <= best_rmse\nfinal_log = test_stack_log if use_stack else blend_test_log\nchoice = \"STACKED (Ridge meta-model)\" if use_stack else \"OPTIMIZED BLEND (grid search)\"\nprint(f\"\\nChosen finalizer: {choice}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Back-transform & save submission\n\nfinal_pred = np.expm1(final_log)  # inverse of log1p\n\nsubmission = pd.DataFrame({\"Id\": test_ids, \"SalePrice\": final_pred})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"✅ submission.csv saved.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-15T21:13:53.288219Z","iopub.execute_input":"2025-08-15T21:13:53.288538Z","iopub.status.idle":"2025-08-15T21:20:25.332744Z","shell.execute_reply.started":"2025-08-15T21:13:53.288516Z","shell.execute_reply":"2025-08-15T21:20:25.331137Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\n--- Fold 1 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000928 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4587\n[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 171\n[LightGBM] [Info] Start training from score 12.023838\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[268]\tvalid_0's l2: 0.00931374\n\n--- Fold 2 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000907 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4591\n[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 171\n[LightGBM] [Info] Start training from score 12.025222\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[431]\tvalid_0's l2: 0.015207\n\n--- Fold 3 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000975 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4582\n[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 171\n[LightGBM] [Info] Start training from score 12.024459\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[215]\tvalid_0's l2: 0.0234593\n\n--- Fold 4 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/848316609.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfold_idx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_idx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             nn.fit(X_tr, y_tr, validation_data=(X_va, y_va), epochs=500, batch_size=32, verbose=0,\n\u001b[0m\u001b[1;32m    159\u001b[0m                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n\u001b[1;32m    160\u001b[0m                               tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=1e-6)])\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":null}]}